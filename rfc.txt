        RFC                          Scaling HomepageContentProvidersCache
        AuthorName(s)
                                      Gabriel Preti Allan Lima
        Contributors
        AuthorDate
                                      May 31, 2023
        DueDate
        DocumentStatus               Draft / In review / Proposed
                                     This document is intended to discuss strategies to sustainable increase the
        Abstract
                                     hit rate in the etag cache used by Moon (Homepage) when calling Content
                                     Providers.
                                     DocumentChangeHistory
        Date           Changedby            ChangeDescription
                                            Initial version
         May 31, 20…    Gabriel Preti
                                            Improving Goal description and adding Open Questions
         2023-06-01     Gabriel Preti
              Context
                    Moonistheservice responsible for calling all the dozens of homepage content providers
              andaggregating their responses to make content decisions for the homepage. Moon has an
              etag based cache when calling the providers, implemented in the common-cached-http library.
              That cache uses a Elasticache/Redis cluster as the storage mechanism, and the keys in that
              cache have a random expiration time between 15 and 20 days. Details of this cache can be
              seen here (although the document mentions Shore, it's currently applied to Moon).
            Themainpurposeofthis cache is to provide failover and increase the efficiency of the
        homepage/content providers ecosystem, but we also have plans of using it as a approximation
        for customer eligibility to widgets, allowing us to have better graceful degradation mechanism for
        the homepage widgets, mainly those more important widgets like CC, Conta and PIX. For this, it
        would be important to have a high hit rate in this cache, ideally we should be able to
        accommodateentries for 100% of customers.
            Recently we made an analysis of the effectiveness of this cache, and we found that its
        hit rate is around 40% and 45%, way lower than we would expect. This document is intended to
        deepdive in the causes for this low hit rate and possible solutions.
        Goal
            Achieve a hit rate of 100% for our active customers in our etag cache under normal
        traffic conditions and with sustainable costs.
            Byactive customers, in this context, we mean customers who accessed the homepage
        in the cache expiration time window (currently a random value between 15 and 20 days) - note
        that this is not Nubank's overall definition of active customer. This can be measured through
        Evictions: if we don't have items leaving the cache due to memory pressure, then we were able
        to accommodate all our active customers.
            Bynormaltraffic conditions we are saying that we don't necessarily want to keep the No
        Evictions premise under abnormal traffic conditions, like sudden traffic spikes for short periods
        of time. Actually, if we indeed want to keep the No Evictions premise under these circumstances
        is an open question that we would like to answer in this document.
            Bysustainable costs we mean something we can decide now that we can pay (whatever
        that value is) and that we can have some predictability on its growth, meaning we don't want our
        costs to go out of control by unexpected events.
        Current Status
            WeanalyzedtheElasticache/Redis clusters in 2 shards: S1 (a smaller shard) and S15 (a
        bigger shard). Both clusters have a setup of 3 cache.r6g.large nodes (13GB memory) with
        Redis Cluster Mode disabled, and in both of them we reached similar conclusions:
          ● Weareusingallavailable memory
          ● Wehaveahitratecloseto45%
          ● Althoughwehave3nodesintheclusters,theclient connections are concentrated in a
            single node. So, the additional nodes are useful only for failover, but they are not helping
            with distribution of the read load.
          ● Wearestoring~5,5MMitemsinthecache.Giventhatwehavelotsof Evictions (items
            leaving the cache due to memory pressure) and no Reclaimed Items (expired items), the
            cache items all leave the cache according to the LRU policy, and we never reach the
            TTLoftheitems.
          Wemadeanexperimentupgradingthenodesizefortheclusters in 2 shards: S0 (a
       smaller shard) and S14 (a bigger shard). In both clusters, we went to a setup of 3
       cache.r6g.xlarge nodes (26GB memory) with cluster mode disabled, and we reached similar
       conclusions in both:
         ● Wearestill using 100% of the memory available.
         ● WehadmarginalimprovementsintheCacheHitRate(somethingaround7pp),even
          though we doubled the available memory.
         ● Wewereabletoroughlydoubletheamountofstoreditems(~11.5MM),whichis
          coherent with doubling the available memory.
         ● WereducedtheamountofEvictions,butwestill see no Reclaimed Items.
          WecanseeintheAppendixthechartsextracted during our analysis
          Weseethattheamountofstoreditemsgoeslinear with the amount of memory, so we
       can estimate what it would take to have all of our customers' data cached in BR. We know that:
         1. Westoreaniteminthecacheforeverycombination of customer and provider endpoint.
         2. Wehave52providerscalled to build the homepage in BR.
         3. Wehave16providerscalled to build the settings page in BR.
          We'll make some assumptions (probably too simplistic, but enough to give us an order of
          magnitude):
         4. Wehave3.5MMcustomerspershard.
         5. Settings and homepage providers' responses have approximately the same size.
         6. All our customers are active, meaning they'll access the homepage at least once in a 15
          days period.
          To store all the homepage data we would need to store 3.5MM customers x 52 providers
       =182MMitems/shardinthecache,whichis roughly 33 times the amount of items we currently
       store for the majority of shards (~431,31GB/shard of memory).
          To store also the settings content, we would need to store 3,5MM x (52 + 16) = 238MM
       items/shard, which is roughly 43 times the amount of items we currently store (~562GB of
       memory).
          Thoseareprobably overestimated: settings providers' responses are smaller than
       homepageproviders, and we don't have 100% of active customers. So, from now on in this doc,
       we'll consider that the estimates we made only for the homepage will be enough to store
       settings + homepage: 33 times the amount of items we currently store for the majority of
       shards (~431,31GB/shard of memory).
       Costs
          Ourcurrent redis cluster doesn't have the cluster mode enabled, meaning that the whole
       data set is replicated to all the nodes in the cluster. So, to increase the amount of items stored
        we'll need to vertically increase the nodes in the cluster: we would need to go from
        cache.r6g.large instances (2 cpu, 13GB memory, $0.41/hour and $897.90/month/shard) to
        cache.r6g.16xlarge (64 cpu, 419 GB, $13.133/hour and $28,761.27/month/shard).
            Thosenumbersarethepublicly available AWS prices for SP region, and doesn't take
        into account Nubank's commercial conditions nor taxes, and closely match On-demand Costs in
        CloudZero. Looking at past months in CloudZero, we see that Billed Costs are roughly 20% of
        OnDemandcosts,soNubankeffectivecostswouldgofrom~$180/month/shardto
        $5,752/month/shard.
            Fromnowoninthisdoc,we'll be more interested in comparing alternatives than with the
        effective cost. So, to keep things simpler, we'll always refer to the publicly available costs we got
        from AWScalculator and not Nubank's effective cost.
        Solutions
          1. Reduce the number of replicas from 3 to 2
            The2readreplicas of our Elasticache/Redis clusters doesn't help with the read load (all
        client connections are concentrated in a single node), so reducing the number of replicas
        wouldn't impact our performance. With two replicas we would still have a failover. This would
        reduce the cost by 33%.
            Wewouldstill be restricted to vertical scaling the nodes if we need more space in the
        future.
          2. Reduce the number of replicas from 3 to 1
            This wouldn't impact performance, but would leave us without failover. This would
        reduce our costs by 67%.
            Wewouldstill be restricted to vertical scaling the nodes if we need more space in the
        future.
          3. Use sharding (Redis cluster mode)
            Elasticache/Redis supports cluster mode, in which the dataset is sharded among the
        instances instead of fully replicated. Each cluster shard is called a Node Group, and Node
        Groups can have replica nodes for increased availability, or it can be composed of a single node
        (if that node is unavailable, all items in that shard will be unavailable).
             Wewon'ttry here to find the best cluster architecture, we'll only present a simple
         reasoning to give us a sense of the numbers.
             It would be possible to accommodate the desired dataset with a cluster composed of 33
         instances of our current cache.r6g.large node type, which would give us a cost of
         $9,876.90/month/shard. Although we would have no failover for the Node Groups, considering
         our current hit rate of 45%, it would take (1 - 0.45) x 33 = 18 nodes failing at the same time to
         bring us to an equivalent hit rate. Having a smaller number of bigger nodes doesn't significantly
         impact the costs.
             Using1yearreservedinstanceswithpartial upfront would bring this cost to
         $6415.97/month/shard.
             Animportant point is that our common-libraries (common-redis) don't support Redis
         Cluster Mode currently. There seems to be no hard reason for that, but we would need to invest
         effort to extend the library. There's a brief discussion here.
           4. Use DDB
             Common-cached-http also supports Dynamo-DB as the cache storage mechanism, so it
         would be simple to start using DDB instead of Redis. Redis offers microseconds latency when
         fetching data, while DDB offers single digits milliseconds - although we are talking about 10x
         difference, it will represent a 10ms increase in latency, which is not significant. Also, it's possible
         to combine DDB with DAX accelerator to mitigate the latency increase, but it will significantly
         boost costs.
             WemadesomecostestimationsusingDDBasthestoragemechanism.Weusedthe
         following numbers in our estimates, which we got from the current Elasticache cluster (we are
         assuming the items occupies the same storage space in DDB and Redis, which we are not
         sure):
           - 431GBofdatastoragesize
           - Average item size of 1,8kb
           - Write settings:
                                      -    Baseline write rate: 1300/sec
                                      -    Peakwrite rate: 3500/sec
                                      -    Duration of peak write activity: 390 hours/month
                                      -    Percentage of baseline writes covered by reserved capacity: 80%
                           -    Readsettings:
                                      -    Baseline read rate: 2500/sec
                                      -    Peakreadrate: 6855/sec
                                      -    Duration of peak read activity: 390 hours/month
                                      -    Percentage of baseline reads covered by reserved capacity: 80%
                                Following are the estimates we obtained:
                           ● Usingon-demandcapacity: $35,599.13 /month/shard
                           ● Usingreservedcapacity 1 year term: $3191.18/month/shard
                           ● Usingreservedcapacity 1 year term and a DAX cluster with 30% of dataset:
                                $6125.08/month/shard.
                                Costs are roughly half of the approach using a sharded Redis if we don't use DAX, and
                      equivalent if we use DAX.
                      Pros/Cons/Fixes Matrix
                                Solutions 1 and 2 doesn't seem to be sustainable in the medium term, so we'll discard
                      themandconsider only solutions 3 (Redis sharded) and 4 (DynamoDB)
                                                                      Pros                                        ConsandFixes
                                                                                                        Cons                                 Fix
                                                          - Smaller latency.                 Moreexpensive
                       Redis Sharded
                                                          - Horizontally Scalable.
                                                          - We pay by cluster size: we
                                                                                             Nodefailures will cause           Alarge number of nodes can
                                                          can easily save $ if we further
                                                                                             cache misses and maybe            alleviate the impact of
                                                          decide we can afford a lower
                                                                                             temporarily instability due to    individual node failures (but
                                                          cache hit rate, meaning we
                                                                                             key redistributions               also increase the probability
                                                          can adapt our costs to our
                                                                                                                               of node failures).
                                                          cache hit needs.
                                                          - We can easily bootstrap a
                                                                                             If we lose an entire availability Wecanhavereplicanodes
                                                          newcluster warmed with
                                                                                             zone, we lose ⅓ of the cache.     for the shards, but the costs
                                                          current cached data (built-in
                                                                                                                               will, at least, double.
                                                          support from AWS).
                                                          - Traffic bursts won't affect the
                                                          costs
                                                                                             Wedon'tknowifalarge
                                                                                             cluster (30+ nodes) will bring
                                                                                             significant operational
                                                                                             burden.
                                                                                             Asthecosts grow with the
                                                                                             size of the dataset, it will
                                                                                             becomemoreexpensive
                                                                                             (more nodes needed) as the
                                                                                             dataset grows (ex: new
                                                                                        providers added).
                                                                                        Ourlibraries doesn't support     Implement the support in
                                                                                        Redis cluster mode               common-redis
                                                       - Cheaper (using reserved        Higher latency                   Canbemitigated with DAX
                      DynamoDB
                                                       capacity)                                                         (but the costs will increase).
                                                       - It's easy to accommodate
                                                       the entire dataset and reach a
                                                                                        Costs can significantly
                                                       100%hitrate.
                                                                                        increase with traffic bursts
                                                       - Costs will only marginally
                                                       increase as the dataset grows
                                                                                        We'll need to constantly         Capacity alarms can alleviate
                                                       (ex: new providers added).
                                                                                        managereservedcapacity           that burden, but won't
                                                                                        allocation                       eliminate it.
                                                                                        Throttles can significantly      Capacity alarms.
                                                                                        impact homepage latency
                               Themostsignificant trade-off seems to be what dimension we want to tie our costs: to
                     storage size (and consequently cache hit rate) or throughput (reads and writes). By choosing
                     Redis we are tying our costs to storage size, and by choosing Dynamo we are tying it to
                     throughput.
                     OpenQuestions
                          1. Do we want to keep the 100% hit rate / no evictions premise
                               at all costs, even under extreme circumstances of sudden
                               traffic spikes? Or do we accept a hit rate degradation under
                               these abnormal circumstances?
                               Keeping the premise will favor the decision toward DynamoDB, as it has less restrictions
                     onthedataset size. However, it will bring us less control over our costs and maybe the stability
                     of our systems, because it will lead us either to spikes of on-demand costs or spike of throttles.
                          2. What is more important to us: guarantee that we'll be able to
                               accommodateall the necessary cache items, or that we
                               have predictability in our costs?
                               If we want to favor the accommodation of all the items, we should favor DDB, as it is way
                     moreflexible regarding dataset size. If we want to accommodate cost control, we should favor
                     Redis, as it'll allow us to fix a dataset size budget, and it will favor generating evictions over cost
                     increase under extreme conditions.
        References
         Magneto - Eng Meeting
        Appendix
           Elasticache cluster charts in S1
           Elasticache cluster charts in S15
       Elasticache cluster charts in S0
       Elasticache cluster charts in S14
       Comparing Moon etag cache with Shore etag cache
          Shore also has an etag cache that stores Moon responses. Although the content of
       those caches are conceptually the same (they both store the widgets sent by the providers, with
       someminordifferences in relation to its size), Shore cache is in a much more comfortable
       situation regarding its size: Hit rate is close to 95%
          We've ran some analysis and we found out that, on average, for a given customer, the
       average size it occupies in Moon's cache is 17x bigger than Shore's cache (although there's a
       hugevariance, with most customers consuming 2,5x space and a few outliers with 1000x). Our
       hypothesis is that this is due to the fact that we currently ask the providers for the widgets for
              every segment (PF, PJ and underaged), although only a subset is responded to Shore (the
              outliers should probably be concentrated in PJ and underaged).
              Legacy widgets
                     We've found some widgets that are returned by the providers, but are never used inside
              Moon(theyprobably are legacy widgets that aren't used anymore): they are 44 widgets (~8,7%
              of the total of widgets inside main-navigation-list):
                                        widget-id                                   location
                                                                         src/facade/logic/shell/content_banner_widg
              BOLETO_FINANCING_NEW_YEAR_HIGHLIGHT                        ets_boleto_financing.clj
              CASHFLOW_ONBOARDING_LENDING_HAVE_CHANGED_WIDGET            src/lagoon/logic/content_widgets.clj
                                                                         src/facade/logic/shell/content_banner_widg
              COLLECTIONS_AND_BILL_CLOSED_HIGHLIGHTS                     ets.clj
              COMPANY_CROSS_SELL                                         src/wingull/logic/shell_widgets/company.clj
              COMPANY_DISCOVER_PURPLE_PAGES                              src/wingull/logic/shell_widgets/company.clj
              COMPANY_DISCOVER_PURPLE_PAGES_ARTWORK_CARD                 src/wingull/logic/shell_widgets/company.clj
              COMPANY_HEADER_PJ_MGM                                      src/wingull/logic/shell_widgets/company.clj
              CREDIT_CARD_AGREEMENT_OFFER                                src/facade/logic/shell/content_widgets.clj
              CREDIT_CARD_BILL_CLOSED                                    src/facade/logic/shell/content_widgets.clj
              CREDIT_CARD_BILL_OPEN                                      src/facade/logic/shell/content_widgets.clj
              CREDIT_CARD_DELINQUENT                                     src/facade/logic/shell/content_widgets.clj
              CREDIT_CARD_DELINQUENT_PAID                                src/facade/logic/shell/content_widgets.clj
              CREDIT_CARD_LATE_BLOCKED                                   src/facade/logic/shell/content_widgets.clj
                                                                         src/nakamoto/api/logic/widgets/cross_sell.c
              CRYPTO_CROSS_SELL_BANNER_CONTENT                           lj
                                                                         src/nakamoto/api/logic/widgets/discover_m
              CRYPTO_PNL_BANNER_WIDGET                                   ore_banner.clj
                                                                         src/mr_wallet/logic/add_to_digital_wallets_
              DIGITAL_WALLETS                                            widget.clj
              DISCOVER_MORE_MARKETPLACE_MOTHERSDAY
              DISCOVER_MORE_SMART_DEFENSES_ARTWORK_CARD
                                                                         src/aparecium/logic/feature_communicatio
              FRAUD_ACTIVATE_SAFE_MODE_BANNER_WIDGET                     n.clj
                                                                         src/aparecium/logic/security_communicatio
              FRAUD_SOS_NU_PROTECTION_PACK_ARTWORK_CARD                  n.clj
                                                                         src/aparecium/logic/security_communicatio
              FRAUD_SOS_NU_STREET_MODE_ARTWORK_CARD                      n.clj
              HIGHLIGHT_CAIXINHAS_Q22023
              HIGHLIGHT_DARK_MODE_SECTION                                src/lagoon/logic/content_widgets.clj
              HIGHLIGHT_MOTHERSDAY_CAMPAIGN
               HIGHLIGHT_PIX-FINANCING-MOTHERSDAY_CAMPAIGN
                                                                           src/cancun/new_investors/content_widgets
               INVESTMENTS_CDB_BANNER                                      /controller.clj
                                                                           src/cancun/onboarding/content_widgets/lo
               INVESTMENTS_LEGACY_TEXT_WIDGET_EMPTY                        gic/main.clj
                                                                           src/cancun/bundles/money_boxes/api/logic
               INVESTMENTS_MONEY_BOXES_HIGHLIGHT_WIDGET                    /content_widget.clj
                                                                           src/cancun/new_investors/content_widgets
               INVESTMENTS_NEW_INVESTORS_FUNDS_WIDGET                      /controller.clj
                                                                           src/cancun/new_investors/content_widgets
               INVESTMENTS_NEW_YEARS_EVE_CDB_HIGHLIGHT                     /controller.clj
               MOISES_PIX_COMMITMENT_REGISTER_WIDGET                       src/moises/api/logic/content_widgets.clj
               OPEN_FINANCE_SECTION_ARTWORK_WIDGET_XP_END                  src/brooklyn_bridge/api/logic/shell.clj#L95
               OPEN_FINANCE_SECTION_ARTWORK_WIDGET_XP_START                src/brooklyn_bridge/api/logic/shell.clj#L93
               OPEN_FINANCE_SECTION_BANNER_WIDGET                          src/brooklyn_bridge/api/logic/shell.clj
               OPEN_FINANCE_SECTION_BANNER_WIDGET_XP_END                   src/brooklyn_bridge/api/logic/shell.clj
               OPEN_FINANCE_SECTION_BANNER_WIDGET_XP_START                 src/brooklyn_bridge/api/logic/shell.clj
               OPEN_FINANCE_SECTION_HIGHLIGHT_WIDGET                       src/brooklyn_bridge/api/logic/shell.clj
                                                                           src/stormshield/br/bundles/phone_topup/co
               PHONE_TOP_UP_HIGHLIGHT_BANNER                               ntent_widgets.clj
               PROFILE_PICTURE_ONBOARDING_BANNER                           src/lagoon/logic/content_widgets.clj
                                                                           src/facade/logic/shell/content_banner_widg
               PURCHASE_FINANCING_SIMULATOR                                ets_boleto_financing.clj
                                                                           src/stormshield/br/bundles/salary_portabilit
               SALARY_PORTABILITY_HIGHLIGHT_WIDGET                         y/content_widget.clj
                                                                           src/facade/logic/shell/content_banner_widg
               TRANSACTION_FINANCING_BANNER                                ets_boleto_financing.clj
                                                                           src/stormshield/br/bundles/whatsapp/logic.
               WHATSAPP_INVITE_WIDGET_POSITION_8                           clj
                                                                           src/stormshield/br/bundles/yield_change_c
               YIELD_CHANGE_COMMUNICATION_ARTWORK                          ommunication/widget/logic.clj
